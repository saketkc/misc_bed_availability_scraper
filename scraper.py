from selenium import webdriver;
from selenium.webdriver.common.by import By
from selenium.webdriver.support.wait import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.webdriver import ActionChains
from tabula import read_pdf
import pandas as pd


import os,requests,time,bs4,datetime,csv,colorama;
from PIL import Image
import json,time,re
from bs4 import BeautifulSoup



# ~ global_proxy='socks4://157.119.201.231:1080'
# ~ global_proxy='socks4://103.88.221.194:46450'
global_proxy='socks4://49.206.195.204:5678'

def get_dataset_from_html_table(table):
  headings = [th.get_text() for th in table.find("tr").find_all("th")]
  datasets = []
  for row in table.find_all("tr")[1:]:
      dataset = list(zip(headings, (td.get_text() for td in row.find_all("td"))))
      datasets.append(dataset)
  return datasets
def highlight(text):
  highlight_begin=colorama.Back.BLACK+colorama.Fore.WHITE+colorama.Style.BRIGHT
  highlight_reset=colorama.Back.RESET+colorama.Fore.RESET+colorama.Style.RESET_ALL
  return highlight_begin+text+highlight_reset

def get_url_failsafe(u,timeout=25):
  x=os.popen('curl --max-time '+str(timeout)+' -# -k '+u).read();
  tries=0
  while (not x) and tries<10: 
    x=os.popen('curl --max-time '+str(2*timeout)+' -x '+global_proxy+' -# -k "'+u+'"').read()
  if x: 
    soup=BeautifulSoup(x,'html.parser')
    return soup
  else:
    print('Failed to download website: %s either directly(curl) or via proxy!!' %(u))
def tamil_nadu_bulletin_parser(bulletin='',return_page_range=False,clip_bulletin=False,return_date=False,dump_clippings=False,return_beds_page=False,return_district_tpr_page=False):
  cmd='pdftotext  -layout "'+bulletin+'" tmp.txt';os.system(cmd)
  # ~ b=[i for i in open('tmp.txt').readlines() if i]
  b=[i for i in open('tmp.txt',encoding='utf-8',errors='ignore').readlines() if i]
  idx=0;page_count=1;page_range=[];got_start=False
  bulletin_date=''
  bd=[i for i in b if 'edia bulletin' in i.lower()]
  bulletin_date_string='';bulletin_date=''
  if bd:
    bulletin_date=bd[0].split('lletin')[1].strip().replace('-','.').replace('/','.')
    bulletin_date_string=bulletin_date
    bulletin_date=datetime.datetime.strptime(bulletin_date,'%d.%m.%Y')
  if return_date: return bulletin_date
    
  for i in b:
    if '\x0c' in i: page_count+=1    
    if return_beds_page and ('BED VACANCY DETAILS'.lower() in i.lower()) : return page_count
    
def tamil_nadu_parse_hospitalizations(bulletin='',use_converted_txt=False,verbose=False):
  date_str=tamil_nadu_bulletin_parser(bulletin,return_date=True).strftime('%Y-%m-%d')
  beds_page=tamil_nadu_bulletin_parser(bulletin,return_beds_page=True)   
  if not use_converted_txt:       
    #hospitalization page
    os.system('pdftotext -layout -f %d -l %d %s tmp.txt' %(beds_page,beds_page,bulletin))
    b=[i.strip() for i in open('tmp.txt').readlines() if i.strip()]
  else: #if forcing, use tmp2.txt
    b=[i.strip() for i in open('tmp2.txt').readlines() if i.strip()]
  start_idx=[i for i in range(len(b)) if 'Ariyalur' in b[i]][0]
  end_idx=[i for i in range(len(b)) if 'Virudhunagar' in b[i]][0]
  bb=b[start_idx:end_idx+2][:-1]
  last_line=[i for i in range(len(b)) if 'grand' in b[i].lower() and 'total' in b[i].lower()][0]
  if len(b[last_line].split())>3: last_line=b[last_line]
  else: last_line='Grand Total '+b[last_line+1]
  data={}
  for i in bb:
    try:
      district,tot_o2,tot_nono2,tot_icu,occ_o2,occ_nono2,occ_icu,vac_o2,vac_nono2,vac_icu,vac_net=i.split()[1:]
    except:
      print('unable to parse hosp page details for line: '+i+'in bulletin: '+bulletin+'\nRreutnring')
      return (i,bb)
    data[district]=[tot_o2,tot_nono2,tot_icu,occ_o2,occ_nono2,occ_icu]
  tot_o2,tot_nono2,tot_icu,occ_o2,occ_nono2,occ_icu,vac_o2,vac_nono2,vac_icu,vac_net=last_line.split()[2:]
  data['All']=[tot_o2,tot_nono2,tot_icu,occ_o2,occ_nono2,occ_icu]
  # ~ if verbose: pprint.pprint(data)
  #CCC page
  os.system('pdftotext -layout -f %d -l %d %s tmp.txt' %(beds_page+1,beds_page+1,bulletin))
  b=[i.strip() for i in open('tmp.txt').readlines() if i.strip()]
  start_idx=[i for i in range(len(b)) if 'Ariyalur' in b[i]][0]
  end_idx=[i for i in range(len(b)) if 'Virudhunagar' in b[i]][0]
  bb=b[start_idx:end_idx+2][:-1]
  last_line=[i for i in range(len(b)) if 'grand' in b[i].lower() and 'total' in b[i].lower()][0]
  if len(b[last_line].split())>4: last_line=b[last_line];
  else: last_line='Grand Total '+b[last_line+1]
  for i in bb:
    district,tot_ccc,occ_ccc,vac_ccc=i.split()[1:]
    data[district].extend([tot_ccc,occ_ccc])
  # ~ print(last_line)
  tot_ccc,occ_ccc,vac_ccc=last_line.split()[2:]
  data['All'].extend([tot_ccc,occ_ccc])
  if verbose: pprint.pprint(data)
  data2=[]
  for district in data:
    if not district=='All':
      x=data[district]
      data2.append([date_str,district,x[0],x[1],x[2],x[6],x[3],x[4],x[5],x[7]])
  x=data['All'];data2.append([date_str,'All',x[0],x[1],x[2],x[6],x[3],x[4],x[5],x[7]])
  a=open('hosp.csv','a')
  w=csv.writer(a)
  for i in data2: w.writerow(i)
  a.close()
  return (data2,date_str)
  
def tamil_nadu_auto_parse_latest_bulletin():
  print('Downloading TN bulletin portal webpage')
  x=os.popen('curl -# -k https://stopcorona.tn.gov.in/daily-bulletin/').read()
  soup=BeautifulSoup(x,'html.parser');  x=soup('div',attrs={'class':'information'})
  if not x:    print('could not find information div in TN bulletin portal!!');    return
  latest_bulletin_url=x[0]('li')[0]('a')[0]['href'].replace('http://','https://')
  cmd='wget --user-agent "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_10_1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/39.0.2171.95 Safari/537.36" "'+latest_bulletin_url+'"'
  print(cmd);os.system(cmd)
  pdf=[i for i in os.listdir('.') if i.endswith('.pdf')];
  if pdf: pdf=pdf[0]
  data,bulletin_date=tamil_nadu_parse_hospitalizations(pdf)
  #check if data for date already exists in csv. if not, then add
  a=open('tamil_nadu.csv');r=csv.reader(a);info=[i for i in r];a.close()
  dates=list(set([i[0] for i in info[1:] if len(i)>0]));dates.sort()
  if bulletin_date not in dates:  os.system('cat hosp.csv >> tamil_nadu.csv && rm -v hosp.csv '+pdf)
  else: print('data for '+bulletin_date+' already existed in tamil_nadu.csv. Only printing, not writing');[print(i) for i in data]
  os.system('rm -vf *.pdf')
 
def gurugram_bulletin_parser(bulletin=''):
  os.system('pdftotext -layout "'+bulletin+'" t.txt')
  b=[i.strip() for i in open('t.txt').readlines() if i.strip()]
 
  x=[i for i in b if 'Dated' in i]
  if not x: print('could not get date in '+bulletin);return  
  x=x[0].split('Dated')[-1].strip().split()[-1]
  x=x.replace('-','/').replace('/04/10/2021','04/10/2021')
  date=datetime.datetime.strptime(x,'%d/%m/%Y').strftime('%Y-%m-%d')

 
  x=[i for i in b if 'found Negative' in i]
  x2=[i for i in b if 'found Positive' in i]
  if not x: print('could not get tests in '+bulletin);return
  tot_tests_to_date=int(x[0].split()[-1].strip())+int(x2[0].split()[-1].strip())
 
  x=[i for i in b if 'New Cases' in i]
  if not x: print('could not get new cases in '+bulletin);return
  cases=int(x[0].split()[-1].strip())
  
  # ~ tpr='%.2f' %(100*(float(cases)/tests));tpr=float(tpr)
  
  x=[i for i in b if '(DCH )' in i]
  if not x: #some 2021 bulletins report combined value
    x=[i for i in b if '(DCH +DCHC)' in i]
    if not x: print('could not get DHC occupancy in '+bulletin);return
    dhc_dchc_occupied=int(x[0].split()[-1].strip())
  else:
    dhc_dchc_occupied=int(x[0].split()[-1].strip())  
    x=[i for i in b if '(DCHC)' in i]
    if not x: print('could not get DCHC occupancy in '+bulletin);return
    # ~ dchc_occupied=int(x[0].split()[-1].strip())
    dhc_dchc_occupied+=int(x[0].split()[-1].strip())
  
  x=[i for i in b if '(DCCC)' in i]
  if not x: print('could not get DCCC occupancy in '+bulletin);return
  dccc_occupied=int(x[0].split()[-1].strip())
  
  x=[i for i in b if 'Home Isolation' in i]
  if not x: print('could not get Home isolation numbers in '+bulletin);return
  home_isolation=int(x[0].split()[-1].strip())
  
  # ~ return (date,cases,tot_tests_to_date,dhc_occupied,dchc_occupied,dccc_occupied,home_isolation)
  return (date,cases,tot_tests_to_date,dhc_dchc_occupied,dccc_occupied,home_isolation)
  
def gurugram_auto_parse_latest_bulletin():
  print('Downloading gurugram bulletin portal webpage')
  x=os.popen('curl -# -k https://gurugram.gov.in/health-bulletin/').read()
  soup=BeautifulSoup(x,'html.parser');x=soup('div',attrs={'class':'status-publish'})
  if not len(x)>0: print('cold not find div from gurugram bulletin portal!!');return
  latest_bulletin_url=x[0]('li')[0]('a')[0]['href']
  cmd='wget --user-agent "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_10_1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/39.0.2171.95 Safari/537.36" "'+latest_bulletin_url+'"'
  print(cmd);os.system(cmd)
  pdf=[i for i in os.listdir('.') if i.endswith('.pdf')];
  if pdf: pdf=pdf[0]
  row=gurugram_bulletin_parser(pdf);bulletin_date=row[0]
  
  #check if data for date already exists in csv. if not, then add
  a=open('gurugram.csv');r=csv.reader(a);info=[i for i in r];a.close()
  dates=list(set([i[0] for i in info[1:] if len(i)>0]));dates.sort()
  if bulletin_date not in dates:  a=open('gurugram.csv','a');w=csv.writer(a);w.writerow(row);a.close()
  else: print('data for '+bulletin_date+' already existed in gurugram.csv. Only printing, not writing');print(row)
  os.system('rm -v "'+pdf+'" *.pdf')

def mumbai_bulletin_auto_parser(bulletin='',proxy=global_proxy):  
  if not bulletin: #download latest bulletin
    # ~ cmd='wget --no-check-certificate --user-agent "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_10_1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/39.0.2171.95 Safari/537.36" "https://stopcoronavirus.mcgm.gov.in/assets/docs/Dashboard.pdf"'
    cmd='curl -# --max-time 15  -O -# -k -A "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_10_1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/39.0.2171.95 Safari/537.36" "https://stopcoronavirus.mcgm.gov.in/assets/docs/Dashboard.pdf"'
    print(cmd);os.system(cmd)
    if os.path.exists('Dashboard.pdf'): bulletin='Dashboard.pdf'
 
  max_tries=100;tries=0
  if os.path.exists(bulletin): 
    print('todays bulletin already exists.nothing to download')
  else:
    while (not os.path.exists(bulletin)) and (tries<max_tries):    
      cmd='curl -#  -O  -k -x "'+proxy+'" "https://stopcoronavirus.mcgm.gov.in/assets/docs/Dashboard.pdf"'
      print(cmd);    os.system(cmd)
      if os.path.exists('Dashboard.pdf'): bulletin='Dashboard.pdf' #download through proxy worked

      os.system('ls -a *.pdf')
      tries+=1
    if os.path.exists('Dashboard.pdf'): bulletin='Dashboard.pdf' #download through proxy worked
      
  #get date
  cmd='pdftotext -x 10 -y 150 -W 200 -H 200 -layout -f 1 -l 1  "'+bulletin+'" t.txt';os.system(cmd)
  b=[i.strip().replace(',','') for i in open('t.txt').readlines() if i.strip()]

  date=[i.replace(',','') for i in b if '2021' in i or '2022' in i]
  if not date: print('could not get date from '+bulletin)
  else: date=datetime.datetime.strptime(date[0],'%b %d %Y');date_str=date.strftime('%Y-%m-%d')
  
  #get cases,tests,symp etc
  cmd='pdftotext -x 0 -y 100 -W 220 -H 320 -layout -f 2 -l 2 "'+bulletin+'" t.txt';os.system(cmd)
  b=[i.strip().replace(',','') for i in open('t.txt').readlines() if i.strip()]
  
  cases=[i for i in b if 'positive' in i.lower()]
  if not cases: print('could not get cases from '+bulletin)
  else: cases=int(cases[0].split()[-1].strip())
  
  active=[i for i in b if 'active' in i.lower()]
  if not active: print('could not get actives from '+bulletin)
  else: active=int(active[0].split()[-1].strip())
  
  asymp=[i for i in b if 'Asymptomatic' in i]
  if not asymp: print('could not get asymp from '+bulletin)
  else: asymp=int(asymp[0].split()[-1].strip())
  
  symp=[i for i in b if 'Symptomatic' in i]
  if not symp: print('could not get symp from '+bulletin)
  else: symp=int(symp[0].split()[-1].strip())
  
  critical=[i for i in b if 'critical' in i.lower()]
  if not critical: print('could not get criticals from '+bulletin)
  else: critical=int(critical[0].split()[-1].strip())
  
  tests=[i for i in b if 'tests' in i.lower()]
  if not tests: print('could not get tests from '+bulletin)
  else: tests=int(tests[0].split()[-1].strip())
  
  #get hospital occupancy
  cmd='pdftotext -x 340 -y 100 -W 95 -H 340 -layout -f 2 -l 2 "'+bulletin+'" t.txt';os.system(cmd)
  b=[i.strip().replace(',','') for i in open('t.txt').readlines() if i.strip()]
  
  if not ('2021' in b[0] or '2022' in b[0]): #means date wasn't at top, parsed wrong
    print('could not parse occupancy numbers')
  else:
    try:
      bc,bo,ba,dc,do,da,oc,oo,oa,ic,io,ia,vc,vo,va=b[1:]
    except:
      print('failed to get occupancy split')
      return b
    gen_beds_cap=int(bc);gen_beds_occupancy=int(bo)
    o2_cap=int(oc);o2_occupancy=int(oo)
    icu_cap=int(ic);icu_occupancy=int(io)
    vent_cap=int(vc);vent_occupancy=int(vo)
 
  row=(date_str,cases,tests,o2_cap,icu_cap,vent_cap,o2_occupancy,icu_occupancy,vent_occupancy,gen_beds_cap,gen_beds_occupancy,active,symp,asymp,critical)
  # ~ a=open('tmp.csv','a');w=csv.writer(a);w.writerow(row);a.close()
  
   #check if data for date already exists in csv. if not, then add
  a=open('mumbai.csv');r=csv.reader(a);info=[i for i in r];a.close()
  dates=list(set([i[0] for i in info[1:] if len(i)>0]));dates.sort()
  print('Mumbai data:')
  if date_str not in dates:  a=open('mumbai.csv','a');w=csv.writer(a);w.writerow(row);a.close()
  else: print('data for '+date_str+' already existed in mumbai.csv. Only printing, not writing');
  print(row)
  
  os.system('rm -v "'+bulletin+'"')
  # ~ return b
  return row

if __name__=='__main__':
  
  
  failed_cities=[]
  # ~ for city in ['bengaluru','hp','mp','chennai','pune','delhi','gbn','gurugram','tn','mumbai','chandigarh','uttarakhand','kerala','ap','telangana','nagpur','nashik','gandhinagar','vadodara','wb','pb','jammu','goa','bihar','rajasthan','ludhiana','jamshedpur']:
  for city in ['mumbai']:
      print('running scraper for: '+city)
      date=datetime.datetime.now();date_str=date.strftime('%Y-%m-%d')
    # ~ try:
      if city=='bengaluru':
        #BENGALURU
  
        url = "https://apps.bbmpgov.in/Covid19/en/mediabulletin.php"
  
        response = requests.get(url)
        soup = BeautifulSoup(response.text, 'html.parser')
        links = soup.find_all('a')
          
        for link in links:
            if ('.pdf' in link.get('href', [])):
                print("Downloading pdf...")
  
                l = "https://apps.bbmpgov.in/Covid19/en/"+link.get('href').replace(" ","%20")
                print(l)
                response = requests.get(l)
                pdf = open("BLR_"+str(date_str)+".pdf", 'wb')
                pdf.write(response.content)
                pdf.close()
                break
        #get date from bulletin
        os.system('pdftotext -layout  BLR_'+str(date_str)+'.pdf t.txt')
        b=[i.strip() for i in open('t.txt').readlines() if i.strip()]
        date_line=[i for i in b if 'WAR ROOM'.lower() in i.lower()]
        if not date_line: print('could not get date from bengaluru buletin BLR_'+str(date_str)+'.pdf !!');sys.exit(1)
        bulletin_date=datetime.datetime.strptime(date_line[0].split('/')[-2].strip(),'%d.%m.%Y').strftime('%Y-%m-%d')
        
        #get page for bed status
        page_count=0;beds_page=0;b=[i for i in open('t.txt').readlines() if i.strip()]
        for i in b:
          if '\x0c' in i: page_count+=1    
          if 'COVID BED STATUS'.lower() in i.lower(): beds_page=page_count+1;break
        # ~ print(beds_page)
    
        # print(text)
        tables = read_pdf("BLR_"+str(date_str)+".pdf", pages=beds_page,silent=True)
        dff=tables[0]
        
        results=[]
        raw_line=' '.join([str(i) for i in list(dff.iloc[len(dff)-1])])
        x=[i for i in raw_line.split() if i.isnumeric()]
        general_available,hdu_available,icu_available,ventilator_available=x[1:5]
        general_admitted,hdu_admitted,icu_admitted,ventilator_admitted=x[6:10]
  
        a=open('data.bengaluru.csv');r=csv.reader(a);info=[i for i in r];a.close()
        dates=list(set([i[0] for i in info[1:]]));dates.sort()
        
        info=', '.join((bulletin_date,str(general_available),str(general_admitted),str(hdu_available),str(hdu_admitted),str(icu_available),str(icu_admitted),str(ventilator_available),str(ventilator_admitted)))        
        
        os.system('rm -vf BLR_'+str(date_str)+'.pdf *.pdf')
        if bulletin_date in dates: 
          # ~ dont_update_data_csv=True
          print('----------\n\nData for %s already exists in data.bengaluru.csv!!\nOnly printing, not modifying csv!!\n\n----------\n\n' %(date_str))
          print('bengaluru: '+str(info))
        else:
          #write to file
          
          a=open('data.bengaluru.csv','a');a.write(info+'\n');a.close()
          print('Appended to data.bengaluru.csv: '+info) 
          
      elif city=='pb':
        soup=get_url_failsafe('https://phsc.punjab.gov.in/en/covid-19-notifications')
        links=[i['href'] for i in soup('a') if i.has_attr('href') and '.xlsx' in i['href']]
        link_date=[i.text for i in soup('a') if i.has_attr('href') and '.xlsx' in i['href']][0]
        date_str=datetime.datetime.strptime(link_date.split()[link_date.split().index('on')+1],'%d-%m-%Y').strftime('%Y-%m-%d')


        os.system('curl -# -k "'+links[0]+'" -o tmp.xlsx')
        os.system('ssconvert tmp.xlsx tmp.csv')
        x=pd.read_csv('tmp.csv');
        summary=list(x.iloc[len(x)-1][3:-4])
        tot_o2=int(summary[0]);      tot_icu=int(summary[8]);      tot_vent=int(summary[13])
        occupied_normal=int(summary[3])+int(summary[5])
        occupied_o2=int(summary[0])-int(summary[1])
        occupied_icu=int(summary[8])-int(summary[9])
        occupied_vent=int(summary[13])-int(summary[14])
        
        os.system('curl -# -k "'+links[1]+'" -o tmp.xlsx')
        os.system('ssconvert tmp.xlsx tmp.csv')
        x=pd.read_csv('tmp.csv');
        summary=list(x.iloc[len(x)-1][3:-4])
        tot_o2+=int(summary[0]);      tot_icu+=int(summary[8]);      tot_vent+=int(summary[13])
        occupied_normal+=int(summary[3])+int(summary[5])
        occupied_o2+=int(summary[0])-int(summary[1])
        occupied_icu+=int(summary[8])-int(summary[9])
        occupied_vent+=int(summary[13])-int(summary[14])
        os.system('rm -vf tmp.csv tmp.xlsx')
        
        row=(date_str,tot_o2,tot_icu,tot_vent,occupied_normal,occupied_o2,occupied_icu,occupied_vent)
        print(city+':');      print(row)
        
      elif city=='tn':
        tamil_nadu_auto_parse_latest_bulletin()
      elif city=='gurugram':
        gurugram_auto_parse_latest_bulletin()
      elif city=='rajasthan':
        soup=get_url_failsafe('https://covidinfo.rajasthan.gov.in/Covid-19hospital-wisebedposition-wholeRajasthan.aspx',75)
        hosp=[' '.join([j.text for j in row('td')]) for row in soup('table')[0]('tr')][3:]
        recent_update=[i for i in hosp if i.split()[-1]!='N/A' and datetime.datetime.strptime(i.split()[-2],'%d-%m-%Y')>=datetime.datetime.now()-datetime.timedelta(days=2) ]
        tot_normal=0;tot_o2=0;tot_icu=0;tot_vent=0;occupied_normal=0;occupied_o2=0;occupied_icu=0;occupied_vent=0
        for i in recent_update:
          tot_normal0,occupied_normal0,x1,tot_o20,occupied_o20,x2,tot_icu0,occupied_icu0,x3,tot_vent0,occupied_vent0,x4=i.split()[-16:-4]
          tot_normal+=int(tot_normal0);tot_o2+=int(tot_o20);tot_icu+=int(tot_icu0);tot_vent+=int(tot_vent0)
          occupied_normal+=int(occupied_normal0);occupied_o2+=int(occupied_o20);occupied_icu+=int(occupied_icu0);occupied_vent+=int(occupied_vent0)
        row=(date_str,tot_normal,tot_o2,tot_icu,tot_vent,occupied_normal,occupied_o2,occupied_icu,occupied_vent)
        print(city+':')
        print(row)
      elif city=='jamshedpur':
        options=webdriver.ChromeOptions();options.add_argument('--ignore-certificate-errors');options.add_argument("--headless") 
        br=webdriver.Chrome(chrome_options=options);br.get('https://xlri.edu/covid19/bed-status/')
        soup=BeautifulSoup(br.page_source,'html.parser')
        cards=soup.select('.card')
        vacant_normal,tot_normal=cards[0]('p')[1].text.split('/')
        occupied_normal=int(tot_normal)-int(vacant_normal)
        vacant_o2,tot_o2=cards[1]('p')[1].text.split('/')
        occupied_o2=int(tot_o2)-int(vacant_o2)
        vacant_icu,tot_icu=cards[2]('p')[1].text.split('/')
        occupied_icu=int(tot_icu)-int(vacant_icu)
        vacant_vent,tot_vent=cards[3]('p')[1].text.split('/')
        occupied_vent=int(tot_vent)-int(vacant_vent)
        row=(date_str,tot_normal,tot_o2,tot_icu,tot_vent,occupied_normal,occupied_o2,occupied_icu,occupied_vent)
        print(city+':')
        print(row)
      elif city=='bihar':
        soup=get_url_failsafe('https://covid19health.bihar.gov.in/DailyDashboard/BedsOccupied',60)
        datasets=get_dataset_from_html_table(soup('table')[0])
        
        
        regularly_updated=['MADHEPURA', 'PATNA', 'BHAGALPUR', 'DARBHANGA', 'MUZAFFARPUR']
        hosp=[i for i in datasets if i[0][1] in regularly_updated and i[3][1]=='DCH']
        
        tot_beds=0;vacant_beds=0;tot_icu=0;vacant_icu=0
        for i in hosp:
          tot_beds+=int(i[5][1])
          vacant_beds+=int(i[6][1])
          tot_icu+=int(i[7][1])
          vacant_icu+=int(i[8][1])
        occupied_beds=tot_beds-vacant_beds
        occupied_icu=tot_icu-vacant_icu
        
        row=(date_str,tot_beds,tot_icu,occupied_beds,occupied_icu)
        print(city+':');      print(row)

      elif city=='gandhinagar':
        x=os.popen('curl --max-time 20 -# -k https://vmc.gov.in/HospitalModuleGMC/Default.aspx').read()
        tries=0
        while (not x) and tries<10: 
          x=os.popen('curl --max-time 60 -x '+global_proxy+' -# -k https://vmc.gov.in/HospitalModuleGMC/Default.aspx').read()
        soup=BeautifulSoup(x,'html.parser')
        x1,x2,x3,vt,vo,vv,it,io,iv,ot,oo,ov,nt,no,nv=[i.text for i in  soup('table')[0]('span') if i.has_attr('id') and i['id'].startswith('lb')]
        row=(date_str,nt,ot,it,vt,no,oo,io,vo)
        print(city+':');      print(row)
  
      elif city=='vadodara':
        x=os.popen('curl --max-time 20 -# -k  https://vmc.gov.in/covid19vadodaraapp/Default.aspx').read()
        tries=0
        while (not x) and tries<10: 
          x=os.popen('curl --max-time 60 -x '+global_proxy+' -# -k https://vmc.gov.in/covid19vadodaraapp/Default.aspx').read()
        soup=BeautifulSoup(x,'html.parser')
        x1,x2,x3,vt,vo,vv,it,io,iv,ot,oo,ov,nt,no,nv,x5=[i.text for i in  soup('table')[0]('span') if i.has_attr('id') and i['id'].startswith('lb')]
        row=(date_str,nt,ot,it,vt,no,oo,io,vo)
        print(city+':');      print(row)
      elif city=='ct':
        pass
      elif city=='wb':
        x=os.popen('curl --max-time 15 -# -k https://excise.wb.gov.in/chms/Portal_Default.aspx').read()
        tries=0
        while (not x) and tries<10: 
          x=os.popen('curl --max-time 60 -x '+global_proxy+' -# -k https://excise.wb.gov.in/chms/Portal_Default.aspx').read()
        soup=BeautifulSoup(x,'html.parser')
        x1,nc,nv,x2=[i.text.strip() for i in  soup('span',attrs={'class':'counter'})]
        no=int(nc)-int(nv)
        row=(date_str,nc,no)
        print(city+':');      print(row)
      elif city=='nashik':
        x=os.popen('curl --max-time 15 -# -k https://covidcbrs.nmc.gov.in/home/hospitalSummary').read()
        tries=0
        while (not x) and tries<10: 
          x=os.popen('curl --max-time 60 -x '+global_proxy+' -# -k https://covidcbrs.nmc.gov.in/home/hospitalSummary').read()
        soup=BeautifulSoup(x,'html.parser')
        x1,x2,x3,x4,nt,nv,ot,ov,it,iv,vt,vv=[i.text.strip() for i in soup('tfoot')[0]('th')]
        no=int(nt)-int(nv)
        oo=int(ot)-int(ov)
        io=int(it)-int(iv)
        vo=int(vt)-int(vv)
        row=(date_str,nt,ot,it,vt,no,oo,io,vo)
        print(city+':');      print(row)
      elif city=='goa':
        soup=get_url_failsafe('https://goaonline.gov.in/beds')
        table=soup('table')[1]
        headings = [th.get_text() for th in table.find("tr").find_all("th")]
        datasets = []
        for row in table.find_all("tr")[1:]:
            dataset = list(zip(headings, (td.get_text() for td in row.find_all("td"))))
            datasets.append(dataset)
        #rest of hosp. not updated
        x=[i for i in datasets if i[1][1] in ["Goa Medical College & Hospital, Bambolim","Victor Hospital, Margao"]]; 
        tot_normal=sum([int(i[2][1]) for i in x])
        vacant_normal=sum([int(i[3][1]) for i in x])
        occupied_normal=tot_normal-vacant_normal
        tot_icu=sum([int(i[4][1]) for i in x])
        vacant_icu=sum([int(i[5][1]) for i in x])
        occupied_icu=tot_icu-vacant_icu
        row=(date_str,tot_normal,tot_icu,occupied_normal,occupied_icu)
        print(city+':');      print(row)
      elif city=='jammu':
        x=os.popen('curl --max-time 30 -# -k https://covidrelief.jk.gov.in/Beds/Hospitals/JAMMU').read()
        tries=0
        while (not x) and tries<10: 
          x=os.popen('curl --max-time 60 -x '+global_proxy+' -# -k https://covidrelief.jk.gov.in/Beds/Hospitals/JAMMU').read()
        soup=BeautifulSoup(x,'html.parser')
        jammu_hospitals=['https://covidrelief.jk.gov.in/Beds/Hospitals/Hospital/609382b4f64c7a2d446721ec','https://covidrelief.jk.gov.in/Beds/Hospitals/Hospital/609381cbb1c6502bfe8c3c5f','https://covidrelief.jk.gov.in/Beds/Hospitals/Hospital/60938338f64c7a2d446721ee','https://covidrelief.jk.gov.in/Beds/Hospitals/Hospital/6093826ef64c7a2d446721eb','https://covidrelief.jk.gov.in/Beds/Hospitals/Hospital/609a4aa4dc9ca218af2fa243','https://covidrelief.jk.gov.in/Beds/Hospitals/Hospital/60bb02f17b6808683a6284e0']
        tnc=tic=tno=too=tio=0
        for hospital in jammu_hospitals:
          x=os.popen('curl --max-time 30 -# -k '+hospital).read()
          tries=0
          while (not x) and tries<10: 
            x=os.popen('curl --max-time 60 -x '+global_proxy+' -# -k '+hospital).read()
          soup=BeautifulSoup(x,'html.parser')
          try:
            x1,x2,x3,nc,nv,ic,iv,oo=[i('td')[1].text for i in soup('table')[0]('tr') if len(i('td'))>1]
            no=int(nc)-int(nv);tno+=no;tnc+=int(nc)
            io=int(ic)-int(iv);tio+=io;tic+=int(ic)
          except:
            print('failed for '+hospital)
            # ~ print(soup)
          
        row=(date_str,tnc,tic,tno,too,tio)
        print(city+':');      print(row)
      elif city=='nagpur':
        # ~ x=os.popen('curl --max-time 30 -# -k https://nsscdcl.org/covidbeds/').read()
        # ~ tries=0
        # ~ while (not x) and tries<10: x=os.popen('curl --max-time 60 -x '+global_proxy+' -# -k https://nsscdcl.org/covidbeds/').read()
          
        soup=get_url_failsafe('https://nsscdcl.org/covidbeds/')      
        oa=soup('div',attrs={'class':'small-box'})[0]('button')[0].text.split(':')[1].strip()
        oo=soup('div',attrs={'class':'small-box'})[0]('label')[0].text.split(':')[1].strip()
        oc=int(oa)+int(oo)
  
        na=soup('div',attrs={'class':'small-box'})[1]('button')[0].text.split(':')[1].strip()
        no=soup('div',attrs={'class':'small-box'})[1]('label')[0].text.split(':')[1].strip()
        nc=int(na)+int(no)
  
        ia=soup('div',attrs={'class':'small-box'})[2]('button')[0].text.split(':')[1].strip()
        io=soup('div',attrs={'class':'small-box'})[2]('label')[0].text.split(':')[1].strip()
        ic=int(ia)+int(io)
  
        va=soup('div',attrs={'class':'small-box'})[3]('button')[0].text.split(':')[1].strip()
        vo=soup('div',attrs={'class':'small-box'})[3]('label')[0].text.split(':')[1].strip()
        vc=int(va)+int(vo)
  
        row=(date_str,nc,oc,ic,vc,no,oo,io,vo)
        print(city+':');      print(row)
      elif city=='mumbai':
        mumbai_bulletin_auto_parser()
      elif city=='gbn':
        #check if data for given date already exists in csv. Update only if data doesn't exist
        a=open('data.gbn.csv');r=csv.reader(a);info=[i for i in r];a.close()
        dates=list(set([i[0] for i in info[1:] if len(i)>0]));dates.sort()
        
        dont_update_data_csv=False
        if date_str in dates: 
          dont_update_data_csv=True
          print('----------\n\nData for %s already exists in csv!!\nOnly printing, not modifying csv!!\n\n----------\n\n' %(date_str))
       
        #get data
        import requests
        from requests.structures import CaseInsensitiveDict
      
        url = "https://api.gbncovidtracker.in/hospitals"
        
        headers = CaseInsensitiveDict()
        headers["Connection"] = "keep-alive"
        headers["Accept"] = "application/json, text/plain, */*"
        headers["DNT"] = "1"
        headers["sec-ch-ua-mobile"] = "?0"
        headers["User-Agent"] = "Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/96.0.4664.45 Safari/537.36"
        headers["sec-ch-ua-platform"] = "Linux"
        headers["Origin"] = "https://gbncovidtracker.in"
        headers["Sec-Fetch-Site"] = "same-site"
        headers["Sec-Fetch-Mode"] = "cors"
        headers["Sec-Fetch-Dest"] = "empty"
        headers["Referer"] = "https://gbncovidtracker.in/"
        headers["Accept-Language"] = "en-US,en;q=0.9"
        
        
        resp = requests.get(url, headers=headers)
        # ~ print('api call status code: ', resp.status_code)
        
        y=resp.json()
        
        if y:
          tot_beds=0;tot_o2_beds=0;tot_ventilator_beds=0;
          occupied_beds=0;occupied_o2_beds=0;occupied_ventilator_beds=0;
          
          for i in y:
            tot_beds+=int(i['normal'])
            tot_o2_beds+=int(i['oxygen'])
            tot_ventilator_beds+=int(i['ventilator'])
            occupied_beds+=(int(i['normal'])-int(i['Vacant_normal']))
            occupied_o2_beds+=(int(i['oxygen'])-int(i['Vacant_oxygen']))
            occupied_ventilator_beds+=(int(i['ventilator'])-int(i['Vacant_ventilator']))
            
          
          # ~ for bed_type in ['beds', 'oxygen_beds', 'covid_icu_beds', 'ventilators', 'icu_beds_without_ventilator', 'noncovid_icu_beds']:
          info='%s,%d,%d,%d,%d,%d,%d\n' %(date_str,tot_beds,tot_o2_beds,tot_ventilator_beds,occupied_beds,occupied_o2_beds,occupied_ventilator_beds)
          
          #write to file
          a=open('data.gbn.csv','a')
          if not dont_update_data_csv:
            a.write(info+'\n')
          print('gbn: '+info)
          a.close()
        else:
          print('could not get data from https://api.gbncovidtracker.in/hospitals')
      
      elif city=='delhi':
        #check if data for given date already exists in csv. Update only if data doesn't exist
        a=open('data.delhi.csv');r=csv.reader(a);info=[i for i in r];a.close()
        dates=list(set([i[0] for i in info[1:]]));dates.sort()
        
        dont_update_data_csv=False
        if date_str in dates: 
              dont_update_data_csv=True
              print('----------\n\nData for %s already exists in data.delhi.csv!!\nOnly printing, not modifying csv!!\n\n----------\n\n' %(date_str))
        
        #get data
        y=str(requests.get('https://coronabeds.jantasamvad.org/covid-info.js').content);
        if y:
              y=json.loads(y[y.find('{'):y.rfind('}')+1].replace('\\n','').replace("\\'",''))
              info=''
              
              # ~ for bed_type in ['beds', 'oxygen_beds', 'covid_icu_beds', 'ventilators', 'icu_beds_without_ventilator', 'noncovid_icu_beds']:
              # ~ info+='%s,%s,%d,%d,%d\n' %(date_str,bed_type,y[bed_type]['All']['total'],y[bed_type]['All']['occupied'],y[bed_type]['All']['vacant'])
              info+='%s,%d,%d,%d,%d,%d,%d,%d,%d,%d,%d,%d,%d' %(date_str,y['beds']['All']['total'],y['oxygen_beds']['All']['total'],y['covid_icu_beds']['All']['total'],y['ventilators']['All']['total'],y['icu_beds_without_ventilator']['All']['total'],y['noncovid_icu_beds']['All']['total'],y['beds']['All']['occupied'],y['oxygen_beds']['All']['occupied'],y['covid_icu_beds']['All']['occupied'],y['ventilators']['All']['occupied'],y['icu_beds_without_ventilator']['All']['occupied'],y['noncovid_icu_beds']['All']['occupied'])    
              print('delhi: '+info)
  
              #write to file            
              if not dont_update_data_csv: a=open('data.delhi.csv','a');a.write(info+'\n');a.close()
        else:
              print('could not get data from https://coronabeds.jantasamvad.org/covid-info.js')
  
      elif city=='pune':
        x=os.popen('curl -# -k https://divcommpunecovid.com/ccsbeddashboard/hsr').read()      
        soup=BeautifulSoup(x,'html.parser');
        xx=soup('legend')[1].parent
        xx=xx('table')[0]
        tot_beds,vacant_beds,tot_normal,vacant_normal,tot_o2,vacant_o2,tot_icu,vacant_icu,tot_vent,vacant_vent=[i.text for i in xx('td') if i.text.isnumeric()]
        print(tot_beds,tot_normal,tot_o2,tot_icu,tot_vent,vacant_beds,vacant_normal,vacant_o2,vacant_icu,vacant_vent)
        occupied_normal=int(tot_normal)-int(vacant_normal)
        occupied_o2=int(tot_o2)-int(vacant_o2)
        occupied_icu=int(tot_icu)-int(vacant_icu)
        occupied_vent=int(tot_vent)-int(vacant_vent)
        row=(date_str,tot_normal,tot_o2,tot_icu,tot_vent,occupied_normal,occupied_o2,occupied_icu,occupied_vent)
        print(city+':')
        print(row)
      elif city=='ap':
        try:
          br=webdriver.PhantomJS();br.get('http://dashboard.covid19.ap.gov.in/ims/hospbed_reports//');time.sleep(3)
          x=br.page_source;      soup=BeautifulSoup(x,'html.parser');
          xyz,number_of_hospitals,tot_icu,occupied_icu,vacant_icu,tot_o2,occupied_o2,vacant_o2,tot_normal,occupied_normal,vacant_normal,tot_vent,occupied_vent,vacant_vent,=[i.text for i in soup('tr')[-1]('td')][:14]
          row=(date_str,tot_normal,tot_o2,tot_icu,tot_vent,occupied_normal,occupied_o2,occupied_icu,occupied_vent)
          print(city+':')
          print(row)
        except:
          print('Failed to download/scrape AP data from http://dashboard.covid19.ap.gov.in/ims/hospbed_reports/ !!')
      elif city=='telangana':
        x=os.popen('curl -# -k http://164.100.112.24/SpringMVC/Hospital_Beds_Statistic_Bulletin_citizen.htm').read()
        soup=BeautifulSoup(x,'html.parser')
        try:
          xyz,tot_normal,occupied_normal,vacant_normal,tot_o2,occupied_o2,vacant_o2,tot_icu,occupied_icu,vacant_icu,a1,a2,a3=[i.text for i in soup('tr')[-1]('th')]
        except:
          print('could not unpack '+str([i.text for i in soup('tr')[-1]('th')]))
        row=(date_str,tot_normal,tot_o2,tot_icu,occupied_normal,occupied_o2,occupied_icu)
        print(city+':')
        print(row)
      elif city=='kerala':
        x=os.popen('curl -# -k https://covid19jagratha.kerala.nic.in/home/addHospitalDashBoard').read()
        soup=BeautifulSoup(x,'html.parser');
        
        n=soup('div',attrs={'class':'box'})[1]
        occupied_normal,tot_normal=n('p')[0].text.replace(n('label')[0].text,'').strip().split('/')
        
        n=soup('div',attrs={'class':'box'})[2]
        occupied_icu,tot_icu=n('p')[0].text.replace(n('label')[0].text,'').strip().split('/')
        
        n=soup('div',attrs={'class':'box'})[3]
        occupied_vent,tot_vent=n('p')[0].text.replace(n('label')[0].text,'').strip().split('/')
        
        n=soup('div',attrs={'class':'box'})[4]
        occupied_o2,tot_o2=n('p')[0].text.replace(n('label')[0].text,'').strip().split('/')
        row=(date_str,tot_normal,tot_o2,tot_icu,tot_vent,occupied_normal,occupied_o2,occupied_icu,occupied_vent)
        print(city+':')
        print(row)
        
      elif city=='uttarakhand':
        x=os.popen('curl -# -k https://covid19.uk.gov.in/bedssummary.aspx').read()
        soup=BeautifulSoup(x,'html.parser');
        
        n=soup('div',attrs={'id':'ContentPlaceHolder1_divIsolation'})[0]
        xz1,tot_normal,xz2,vacant_normal=[i.text for i in n('span')];
        occupied_normal=int(tot_normal)-int(vacant_normal)
        
        n=soup('div',attrs={'id':'ContentPlaceHolder1_divOx2'})[0]
        xz1,tot_o2,xz2,vacant_o2=[i.text for i in n('span')];
        occupied_o2=int(tot_o2)-int(vacant_o2)
        
        n=soup('div',attrs={'id':'ContentPlaceHolder1_divICU'})[0]
        xz1,tot_icu,xz2,vacant_icu=[i.text for i in n('span')];
        occupied_icu=int(tot_icu)-int(vacant_icu)
        
        n=soup('div',attrs={'id':'ContentPlaceHolder1_div1'})[0]
        xz1,tot_vent,xz2,vacant_vent=[i.text for i in n('span')];
        occupied_vent=int(tot_vent)-int(vacant_vent)
        
        row=(date_str,tot_normal,tot_o2,tot_icu,tot_vent,occupied_normal,occupied_o2,occupied_icu,occupied_vent)
        print(city+':')
        print(row)
        
      elif city=='chandigarh':
        x=os.popen('curl -# -k http://chdcovid19.in/chdcovidbed19/index.php/home/stats').read()
        soup=BeautifulSoup(x,'html.parser');
        table=soup('table')[0]
        
        # ~ toc=tvc=tic=tnc=0
        # ~ too=tvo=tio=tno=0
        # ~ for row in table('tr')[2:]:
          # ~ hospital_name,hosp_type,updated_on,oc,oa,ov,nc,no,nv,ic,io,iv,vc,vo,vv=[i.text for i in  row('td')]
          # ~ toc+=int(oc);        tvc+=int(vc);        tic+=int(ic);        tnc+=int(nc)
          # ~ too+=int(oo);        tvo+=int(vo);        tio+=int(io);        tno+=int(no)
        
        try: xyz,toc,too,toa,tnc,tno,tna,tic,tio,tia,tvc,tvo,tva=[i.text for i in  table('tr')[-1]('td')]
        except:
          print('could not unpack chandigarh values!\n'+str(table('tr')[-1]('td')))
        row=(date_str,tnc,toc,tic,tvc,tno,too,tio,tvo)
        print(city+' : '+str(row))
      elif city=='hp':
        x=os.popen('curl -# -k https://covidcapacity.hp.gov.in/index.php').read()
        soup=BeautifulSoup(x,'html.parser');
        xx=soup('a',attrs={'id':'oxygenbedmodel'})[0]
        tot_o2=int(xx.parent.parent('td')[0].text)
        occupied_o2=int(xx.parent.parent('td')[1].text)
        xx=soup('a',attrs={'id':'icubedmodel'})[0]
        tot_icu=int(xx.parent.parent('td')[0].text)
        occupied_icu=int(xx.parent.parent('td')[1].text)
        xx=soup('a',attrs={'id':'Standardbedmodel'})[0]
        tot_normal=int(xx.parent.parent('td')[0].text)
        occupied_normal=int(xx.parent.parent('td')[1].text)
        row=(date_str,tot_normal,tot_o2,tot_icu,occupied_normal,occupied_o2,occupied_icu)
        print(city+':');print(row)
      elif city=='mp':
        x=os.popen('curl -# -k http://sarthak.nhmmp.gov.in/covid/facility-bed-occupancy-dashboard/').read()
        soup=BeautifulSoup(x,'html.parser');
        xx=soup('a',attrs={'href':'http://sarthak.nhmmp.gov.in/covid/facility-bed-occupancy-details'})
        tot_normal,occupied_normal,vacant_normal,tot_o2,occupied_o2,vacant_o2,tot_icu,occupied_icu,vacant_icu=[i.text for i in xx if i.text.isnumeric()]
        row=(date_str,tot_normal,tot_o2,tot_icu,occupied_normal,occupied_o2,occupied_icu)
        print(city+':');print(row)
      elif city=='ludhiana':

        soup = get_url_failsafe("https://ludhiana.nic.in/bed-status/")
        links = soup.find_all('a')
        
        for link in links:
          if ('.pdf' in link.get('href', [])):
              print("Downloading pdf...")

              l = link.get('href')
              print(l)
              response = requests.get(l)
              pdf = open("LDH_"+str(date_str)+".pdf", 'wb')
              pdf.write(response.content)
              pdf.close()
              break

        #get date
        os.system('pdftotext -f 1 -l 1 -x 0 -y 0 -W 500 -H 300  -layout LDH_'+str(date_str)+'.pdf tmp.txt')
        b=[i.strip() for i in open('tmp.txt').readlines() if i.strip()]
        date_line=['Last edited on 9-January-2022 5.00 PM']
        if not date_line:         print(highlight('could not extract date for Ludhiana!!'));         continue
        date_line=date_line[0].split();date_line=date_line[date_line.index('on')+1]
        bulletin_date=datetime.datetime.strptime(date_line,'%d-%B-%Y')
       
      # print(text)
        tables = read_pdf("LDH_"+str(date_str)+".pdf", pages="all")
        df=tables[-1]
        print(df.iloc[-1])
        nums = []
        for x in df.iloc[-1]:
          if(type(x) is None):
            continue
          if(type(x) == str):
            for s in x.split():
              if(s.isnumeric()):
                nums.append(s)
  
        # ~ print(nums)
        tot_o2,occupied_o2,vacant_o2,tot_icu,occupied_icu,vacant_icu=nums
        a=open('data.ludhiana.csv');r=csv.reader(a);info=[i for i in r];a.close()
        dates=list(set([i[0] for i in info[1:]]));dates.sort()
        #save space by deleting the pdf
        if os.path.exists("LDH_"+str(date_str)+".pdf"): os.remove("LDH_"+str(date_str)+".pdf")
        date_str=bulletin_date.strftime('%Y-%m-%d')
        row=(date_str,tot_o2,tot_icu,occupied_o2,occupied_icu)
        print(city+':'+str(row))
        # ~ if date_str in dates: 
          # ~ print('----------\n\nData for %s already exists in data.ludhiana.csv!!\nOnly printing, not modifying csv!!\n\n----------\n\n' %(date_str))
        # ~ else:
          # ~ #write to file
          # ~ info=', '.join((date_str,tot_o2,tot_icu,occupied_o2,occupied_icu))
          # ~ print(city+' : '+str(info)) 
          # ~ # Date, L2_Total_Beds, L2_Occupied_Beds, L2_Available_Beds, L3_Total_Beds, L3_Occupied_Beds, L3_Available_Beds
          # ~ a=open('data.ludhiana.csv','a');a.write(info+'\n');a.close()
          # ~ print('Appended to data.ludhiana.csv: '+info) 
      elif city=='chennai':
        #CHENNAI
        import requests
        from requests.structures import CaseInsensitiveDict
        
        url = "https://tncovidbeds.tnega.org/api/hospitals"
        
        headers = CaseInsensitiveDict()
        headers["authority"] = "tncovidbeds.tnega.org"
        #headers["sec-ch-ua"] = "" Not A;Brand";v="99", "Chromium";v="96", "Google Chrome";v="96""
        headers["dnt"] = "1"
        headers["sec-ch-ua-mobile"] = "?0"
        headers["user-agent"] = "Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/96.0.4664.45 Safari/537.36"
        headers["content-type"] = "application/json;charset=UTF-8"
        headers["accept"] = "application/json, text/plain, */*"
        headers["token"] = "null"
        #headers["sec-ch-ua-platform"] = ""Linux""
        headers["sec-ch-ua-platform"] = "Linux"
        headers["origin"] = "https://tncovidbeds.tnega.org"
        headers["sec-fetch-site"] = "same-origin"
        headers["sec-fetch-mode"] = "cors"
        headers["sec-fetch-dest"] = "empty"
        headers["accept-language"] = "en-US,en;q=0.9"
        #headers["cookie"] = "_ga=GA1.2.1493856265.1640076462; _gid=GA1.2.514620938.1640076462; _gat=1"
        
        data = '{"searchString":"","sortCondition":{"Name":1},"pageNumber":1,"pageLimit":200,"SortValue":"Availability","ShowIfVacantOnly":"","Districts":["5ea0abd2d43ec2250a483a40"],"BrowserId":"6f4dfda2b7835796132d69d0e8525127","IsGovernmentHospital":true,"IsPrivateHospital":true,"FacilityTypes":["CHO"]}'
        
        
        resp = requests.post(url, headers=headers, data=data)
        
        print(resp.status_code)
        y=json.loads(resp.content.decode('unicode_escape').replace('\n',''))
        tot_o2_beds=0;tot_non_o2_beds=0;tot_icu_beds=0;
        occupied_o2_beds=0;occupied_non_o2_beds=0;occupied_icu_beds=0;
        vacant_o2_beds=0;vacant_non_o2_beds=0;vacant_icu_beds=0;
        
        for i in y['result']:
          tot_o2_beds+=i['CovidBedDetails']['AllotedO2Beds']
          tot_non_o2_beds+=i['CovidBedDetails']['AllotedNonO2Beds']
          tot_icu_beds+=i['CovidBedDetails']['AllotedICUBeds']
          occupied_o2_beds+=i['CovidBedDetails']['OccupancyO2Beds']
          occupied_non_o2_beds+=i['CovidBedDetails']['OccupancyNonO2Beds']
          occupied_icu_beds+=i['CovidBedDetails']['OccupancyICUBeds']
          vacant_o2_beds+=i['CovidBedDetails']['VaccantO2Beds']
          vacant_non_o2_beds+=i['CovidBedDetails']['VaccantNonO2Beds']
          vacant_icu_beds+=i['CovidBedDetails']['VaccantICUBeds']
        print('In Chennai, on %s\nO2: %d/%d occupied\nNon-O2 %d/%d occupied\nICU: %d/%d occupied' %(date_str,occupied_o2_beds,tot_o2_beds,occupied_non_o2_beds,tot_non_o2_beds,occupied_icu_beds,tot_icu_beds))
        
        
        a=open('data.chennai.csv');r=csv.reader(a);info=[i for i in r];a.close()
        dates=list(set([i[0] for i in info[1:]]));dates.sort()
        
        if date_str in dates: 
          # ~ dont_update_data_csv=True
          print('----------\n\nData for %s already exists in data.chennai.csv!!\nOnly printing, not modifying csv!!\n\n----------\n\n' %(date_str))
        else:
          #write to file
          info=', '.join((date_str,str(tot_o2_beds),str(tot_non_o2_beds),str(tot_icu_beds),str(occupied_o2_beds),str(occupied_non_o2_beds),str(occupied_icu_beds)))        
          a=open('data.chennai.csv','a');a.write(info+'\n');a.close()
          print('Appended to data.chennai.csv: '+info)        
      
      #generic writer for most cities
      if city in ['mp','hp','pune','chandigarh','uttarakhand','kerala','ap','telangana','nagpur','nashik','gandhinagar','vadodara','wb','pb','jammu','goa','bihar','rajasthan','ludhiana','jamshedpur']:
        csv_fname='data.'+city+'.csv'
        a=open(csv_fname);r=csv.reader(a);info=[i for i in r];a.close()
        dates=list(set([i[0] for i in info[1:]]));dates.sort()
        
        if date_str in dates: 
          # ~ dont_update_data_csv=True
          print('----------\n\nData for %s already exists in %s!!\nOnly printing, not modifying csv!!\n\n----------\n\n' %(date_str,csv_fname))
        else:
          #write to file
          a=open(csv_fname,'a');w=csv.writer(a);w.writerow(row);a.close()
          print('Appended to %s :%s' %(csv_fname,str(row)))        
    # ~ except:
      # ~ failed_cities.append(city)
    
  # ~ for city in failed_cities:    print('Failed to run scraper for : '+highlight(city))
    
  
